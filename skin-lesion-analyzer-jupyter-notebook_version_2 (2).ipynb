{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6f77793e7ebbcd523899bafaa9dae9763b9a0a5"
   },
   "source": [
    "**Introduction**\n",
    "\n",
    "We've always wanted to build an end to end machine learning solution - starting with model creation and ending with a live web app. Here we've managed to do it. Users are able to submit a picture of a skin lesion and get an instant prediction. This kernel details the process we followed to build the model and then convert it from Keras to Tensorflow.js. \n",
    "<br>\n",
    "\n",
    "Web App:<br>http://dermatological-classification.herokuapp.com/<br>\n",
    "\n",
    "\n",
    "This model classifies skin lesions into seven classes. It is a fine tuned CNN. All training was done in this kernel. The main challenges were the unbalanced dataset and the small amount of data.  I used data augmentation to reduce the class imbalance and in so doing get categorical accuracy scores that were not heavily skewed by a single majority class.\n",
    "\n",
    "MobileNet’s small size and speed makes it ideal for web deployment. It’s also a joy to train.\n",
    "\n",
    "Tensorflow.js is a new library that allows machine learning models to run in the browser - without having to download or install any additional software. Because the model is running locally, any data that a user submits never leaves his or her pc or mobile phone. I imagine that privacy is especially important when it comes to medical data.\n",
    "\n",
    "<hr>\n",
    "\n",
    "**What is the objective?**\n",
    "\n",
    "We found it very helpful to define a clear objective right at the start. This helps guide the model selection process. For example, if a model has an accuracy of 60% it would usually be seen as a bad model. However, if it also has a top 3 accuracy of 90% and the objective requires that it output 3 predictions then it may actually be quite a good model. \n",
    "\n",
    "*This is the objective that we defined for this task:*\n",
    "\n",
    "> Create an online tool that can tell doctors and lab technologists the three highest probability diagnoses for a given skin lesion. This will help them quickly identify high priority patients and speed up their workflow. The app should produce a result in less than 3 seconds. To ensure privacy the images must be pre-processed and analysed locally and never be uploaded to an external server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "371731306c3e504b191979706e826c247def88dc"
   },
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(101)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(101)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3f6843b78793e1c047ca6909a7449dc9bfc3f1c"
   },
   "source": [
    "**LABELS**<br>\n",
    "\n",
    "Excerpts from the paper:<br>\n",
    "> The HAM10000 Dataset: A Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions<br>\n",
    "https://arxiv.org/abs/1803.10417\n",
    "\n",
    "\n",
    "\n",
    " **nv**<br>\n",
    " Melanocytic nevi are benign neoplasms of melanocytes and appear in a myriad of variants, which all are included in our series. The variants may differ significantly from a dermatoscopic point of view.<br>\n",
    " *[6705 images]*\n",
    " \n",
    " **mel**<br>\n",
    " Melanoma is a malignant neoplasm derived from melanocytes that may appear in different variants. If excised in an early stage it can be cured by simple surgical excision. Melanomas can be invasive or non-invasive (in situ). We included all variants of melanoma including melanoma in situ, but did exclude non-pigmented, subungual, ocular or mucosal melanoma.<br>*[1113 images]*\n",
    " \n",
    " \n",
    "**bkl**<br>\n",
    " \"Benign keratosis\" is a generic class that includes seborrheic ker- atoses (\"senile wart\"), solar lentigo - which can be regarded a flat variant of seborrheic keratosis - and lichen-planus like keratoses (LPLK), which corresponds to a seborrheic keratosis or a solar lentigo with inflammation\n",
    "and regression [22]. The three subgroups may look different dermatoscop- ically, but we grouped them together because they are similar biologically and often reported under the same generic term histopathologically. From a dermatoscopic view, lichen planus-like keratoses are especially challeng- ing because they can show morphologic features mimicking melanoma [23] and are often biopsied or excised for diagnostic reasons.<br>\n",
    "*[1099 images]*\n",
    "\n",
    "**bcc**<br>\n",
    "Basal cell carcinoma is a common variant of epithelial skin cancer that rarely metastasizes but grows destructively if untreated. It appears in different morphologic variants (flat, nodular, pigmented, cystic, etc) [21], which are all included in this set.<br>\n",
    "*[514 images]*\n",
    " \n",
    "**akiec**<br>\n",
    "Actinic Keratoses (Solar Keratoses) and intraepithelial Carcinoma (Bowen’s disease) are common non-invasive, variants of squamous cell car- cinoma that can be treated locally without surgery. Some authors regard them as precursors of squamous cell carcinomas and not as actual carci- nomas. There is, however, agreement that these lesions may progress to invasive squamous cell carcinoma - which is usually not pigmented. Both neoplasms commonly show surface scaling and commonly are devoid of pigment. Actinic keratoses are more common on the face and Bowen’s disease is more common on other body sites. Because both types are in- duced by UV-light the surrounding skin is usually typified by severe sun damaged except in cases of Bowen’s disease that are caused by human papilloma virus infection and not by UV. Pigmented variants exists for Bowen’s disease [19] and for actinic keratoses [20]. Both are included in this set.<br>*[327 images]*\n",
    "\n",
    "\n",
    "**vasc**<br>\n",
    "Vascular skin lesions in the dataset range from cherry angiomas to angiokeratomas [25] and pyogenic granulomas [26]. Hemorrhage is also included in this category.<br>\n",
    "*[142 images]*\n",
    "\n",
    "**df**<br>\n",
    "Dermatofibroma is a benign skin lesion regarded as either a benign proliferation or an inflammatory reaction to minimal trauma. It is brown often showing a central zone of fibrosis dermatoscopically [24].<br>*[115 images]*\n",
    "\n",
    "\n",
    "<br>*[Total images = 10015]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5a0a200bfc57c5489eaa930255d9420a7d01c47"
   },
   "outputs": [],
   "source": [
    "os.listdir('../input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "086162161ba405b800863e7d545b5917e5205984"
   },
   "source": [
    "### Create the directory structure\n",
    "\n",
    "In these folders we will store the images that will later be fed to the Keras generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d24ef21f9f2359b8bf6b3e7a0b8ab5a43daaf566"
   },
   "outputs": [],
   "source": [
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "nv = os.path.join(train_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(train_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(train_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(train_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(train_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(train_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(train_dir, 'df')\n",
    "os.mkdir(df)\n",
    "nv = os.path.join(val_dir, 'nv')\n",
    "os.mkdir(nv)\n",
    "mel = os.path.join(val_dir, 'mel')\n",
    "os.mkdir(mel)\n",
    "bkl = os.path.join(val_dir, 'bkl')\n",
    "os.mkdir(bkl)\n",
    "bcc = os.path.join(val_dir, 'bcc')\n",
    "os.mkdir(bcc)\n",
    "akiec = os.path.join(val_dir, 'akiec')\n",
    "os.mkdir(akiec)\n",
    "vasc = os.path.join(val_dir, 'vasc')\n",
    "os.mkdir(vasc)\n",
    "df = os.path.join(val_dir, 'df')\n",
    "os.mkdir(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ae8d37fdee293aaffa71a79019dd7277f8288fc"
   },
   "source": [
    "### Create Train and Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "268503398ef61904e05a2c0b0667d589f08a19a8"
   },
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../input/HAM10000_metadata.csv')\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c46ea5967e095d31dcf144b6f57f0343878fa432"
   },
   "source": [
    "### Create a stratified val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53e4b7b152ed831a7d7516156ac300c0e6985ffc"
   },
   "outputs": [],
   "source": [
    "# this will tell us how many images are associated with each lesion_id\n",
    "df = df_data.groupby('lesion_id').count()\n",
    "\n",
    "# now we filter out lesion_id's that have only one image associated with it\n",
    "df = df[df['image_id'] == 1]\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "24720fe3ea9f2f4b571abd09ecfbb931d6429852"
   },
   "outputs": [],
   "source": [
    "def identify_duplicates(x):\n",
    "    \n",
    "    unique_list = list(df['lesion_id'])\n",
    "    \n",
    "    if x in unique_list:\n",
    "        return 'no_duplicates'\n",
    "    else:\n",
    "        return 'has_duplicates'\n",
    "df_data['duplicates'] = df_data['lesion_id']\n",
    "df_data['duplicates'] = df_data['duplicates'].apply(identify_duplicates)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08b7eef3e0ac4112f63b8fb26ce19d55483cbc04"
   },
   "outputs": [],
   "source": [
    "df_data['duplicates'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "995445dfda2745165a53e61f42615104b951d4af"
   },
   "outputs": [],
   "source": [
    "df = df_data[df_data['duplicates'] == 'no_duplicates']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "39fde25b59a9452cf700c5b2ff82cc7cc45c4a33"
   },
   "outputs": [],
   "source": [
    "y = df['dx']\n",
    "_, df_val = train_test_split(df, test_size=0.17, random_state=101, stratify=y)\n",
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1df37227f7ce993d054ed5b8480ee724696fc210"
   },
   "outputs": [],
   "source": [
    "df_val['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "08c5e12fcef2da5f49267a6b82161b2c52c2b20a"
   },
   "source": [
    "### Create a train set that excludes images that are in the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03715a6cf5aeb6430ee144a84eb10dde216c0fb9"
   },
   "outputs": [],
   "source": [
    "def identify_val_rows(x):\n",
    "    val_list = list(df_val['image_id'])\n",
    "    if str(x) in val_list:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'train'\n",
    "df_data['train_or_val'] = df_data['image_id']\n",
    "df_data['train_or_val'] = df_data['train_or_val'].apply(identify_val_rows)\n",
    "df_train = df_data[df_data['train_or_val'] == 'train']\n",
    "print(len(df_train))\n",
    "print(len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b976a9018b1bd2dc0522c68339c5861534a1571"
   },
   "outputs": [],
   "source": [
    "df_train['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1581d5a3e86f9673ae175102112017e30229bc37"
   },
   "outputs": [],
   "source": [
    "df_val['dx'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8812ad87c4fa18d2d82497df42c3895c7f10bc39"
   },
   "source": [
    "### Transfer the Images into the Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4acee2b7879762e50b52df118a9b691515fe7ac0"
   },
   "outputs": [],
   "source": [
    "df_data.set_index('image_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eca02fbf066c8124d0cb465295bbd2593f5f045a"
   },
   "outputs": [],
   "source": [
    "folder_1 = os.listdir('../input/ham10000_images_part_1')\n",
    "folder_2 = os.listdir('../input/ham10000_images_part_2')\n",
    "\n",
    "train_list = list(df_train['image_id'])\n",
    "val_list = list(df_val['image_id'])\n",
    "\n",
    "\n",
    "for image in train_list:\n",
    "    \n",
    "    fname = image + '.jpg'\n",
    "    label = df_data.loc[image,'dx']\n",
    "    \n",
    "    if fname in folder_1:\n",
    "        src = os.path.join('../input/ham10000_images_part_1', fname)\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    if fname in folder_2:\n",
    "        src = os.path.join('../input/ham10000_images_part_2', fname)\n",
    "        dst = os.path.join(train_dir, label, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "for image in val_list:\n",
    "    \n",
    "    fname = image + '.jpg'\n",
    "    label = df_data.loc[image,'dx']\n",
    "    \n",
    "    if fname in folder_1:\n",
    "        src = os.path.join('../input/ham10000_images_part_1', fname)\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "\n",
    "    if fname in folder_2:\n",
    "        src = os.path.join('../input/ham10000_images_part_2', fname)\n",
    "        dst = os.path.join(val_dir, label, fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5a4847c4cc799c23e57bf2531d92117cb95e1b07"
   },
   "outputs": [],
   "source": [
    "print(len(os.listdir('base_dir/train_dir/nv')))\n",
    "print(len(os.listdir('base_dir/train_dir/mel')))\n",
    "print(len(os.listdir('base_dir/train_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/train_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/train_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/train_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/train_dir/df')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd05c08cbfa00418dc333f5b67d1ff6e98aa973e"
   },
   "outputs": [],
   "source": [
    "print(len(os.listdir('base_dir/val_dir/nv')))\n",
    "print(len(os.listdir('base_dir/val_dir/mel')))\n",
    "print(len(os.listdir('base_dir/val_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/val_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/val_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/val_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/val_dir/df')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cebcb5242ff542efb03be5086bf3796bea70c591"
   },
   "source": [
    "### Copy the train images  into aug_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fe970d74e9d5a284420af4ad37d8aae89dc1c15"
   },
   "outputs": [],
   "source": [
    "class_list = ['mel','bkl','bcc','akiec','vasc','df']\n",
    "\n",
    "for item in class_list:\n",
    "    \n",
    "\n",
    "    aug_dir = 'aug_dir'\n",
    "    os.mkdir(aug_dir)\n",
    "    img_dir = os.path.join(aug_dir, 'img_dir')\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "    img_class = item\n",
    "\n",
    "    img_list = os.listdir('base_dir/train_dir/' + img_class)\n",
    "\n",
    "    for fname in img_list:\n",
    "            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n",
    "            dst = os.path.join(img_dir, fname)\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "    path = aug_dir\n",
    "    save_path = 'base_dir/train_dir/' + img_class\n",
    "\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    aug_datagen = datagen.flow_from_directory(path,\n",
    "                                           save_to_dir=save_path,\n",
    "                                           save_format='jpg',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "    \n",
    "    num_aug_images_wanted = 6000 \n",
    "    \n",
    "    \n",
    "    num_files = len(os.listdir(img_dir))\n",
    "    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n",
    "\n",
    "    for i in range(0,num_batches):\n",
    "\n",
    "        imgs, labels = next(aug_datagen)\n",
    "        \n",
    "    shutil.rmtree('aug_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9bbc56bd25441150d2430dca2b07d8ebae57d95"
   },
   "outputs": [],
   "source": [
    "print(len(os.listdir('base_dir/train_dir/nv')))\n",
    "print(len(os.listdir('base_dir/train_dir/mel')))\n",
    "print(len(os.listdir('base_dir/train_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/train_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/train_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/train_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/train_dir/df')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21de03bdc63ecf78cc061d364d14d3216a544b43"
   },
   "outputs": [],
   "source": [
    "print(len(os.listdir('base_dir/val_dir/nv')))\n",
    "print(len(os.listdir('base_dir/val_dir/mel')))\n",
    "print(len(os.listdir('base_dir/val_dir/bkl')))\n",
    "print(len(os.listdir('base_dir/val_dir/bcc')))\n",
    "print(len(os.listdir('base_dir/val_dir/akiec')))\n",
    "print(len(os.listdir('base_dir/val_dir/vasc')))\n",
    "print(len(os.listdir('base_dir/val_dir/df')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32dad10b7c104d2baa972da8cbadc7d6038af05c"
   },
   "source": [
    "### Set Up the Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aa1041d69b0e8313324b91e3e9475799e1ad61c2"
   },
   "outputs": [],
   "source": [
    "train_path = 'base_dir/train_dir'\n",
    "valid_path = 'base_dir/val_dir'\n",
    "\n",
    "num_train_samples = len(df_train)\n",
    "num_val_samples = len(df_val)\n",
    "train_batch_size = 10\n",
    "val_batch_size = 10\n",
    "image_size = 224\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d0e5aede7139196b0d4e1344b278e7621f005550"
   },
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    tensorflow.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "train_batches = datagen.flow_from_directory(train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=train_batch_size)\n",
    "\n",
    "valid_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=val_batch_size)\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ee4ee41f1b16083bd9fc20ee9dec40acccc97dd"
   },
   "source": [
    "### Modify MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad582cb8ea0ca2d563fc367aa89b7edfafc1a57f"
   },
   "outputs": [],
   "source": [
    "mobile = tensorflow.keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "960449ec7ecdda92ba733ad23b00b7be605f3d4b"
   },
   "outputs": [],
   "source": [
    "mobile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b7922bdf625675834d9b63ec0e85351bd9f3c0f"
   },
   "outputs": [],
   "source": [
    "type(mobile.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f832e5865c65a013a06dbf5d500c0381020c56d5"
   },
   "outputs": [],
   "source": [
    "len(mobile.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4dd9dcf26d85a57a113e6b158cf8fceeca7f99de"
   },
   "outputs": [],
   "source": [
    "\n",
    "x = mobile.layers[-6].output\n",
    "\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(7, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=mobile.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "b38734b72afc4289ab187a9e683cbda6bf3269bc"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9d74e44630c3d07a596460c8fbfda3ae7cae1e9"
   },
   "outputs": [],
   "source": [
    "\n",
    "for layer in model.layers[:-23]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13cf63a53e5195cb8a9725d2506c71108bc478b9"
   },
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "915f30a4f5ad369713bcb8e3bfa438219d6c8ef7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "\n",
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n",
    "\n",
    "def top_2_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2013ff1abae70fed845af94e7ab3d95cefad0d61"
   },
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n",
    "              metrics=[categorical_accuracy, top_2_accuracy, top_3_accuracy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62e7a784a33d4c868f49a3ef1f9acbc7186e3338"
   },
   "outputs": [],
   "source": [
    "print(valid_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3001857c9a3c2b15c2343627e340eb1ae858fae9"
   },
   "outputs": [],
   "source": [
    "class_weights={\n",
    "    0: 1.0,\n",
    "    1: 1.0, \n",
    "    2: 1.0, \n",
    "    3: 1.0, \n",
    "    4: 3.0, \n",
    "    5: 1.0, \n",
    "    6: 1.0, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "4a5e3bc3cf44f1d4326c34ad880a302ba082e9d5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_top_3_accuracy', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_top_3_accuracy', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "                              \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                              class_weight=class_weights,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=30, verbose=1,\n",
    "                   callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c3e43e3f2943db4be9d75831fe23661ae9deb44b"
   },
   "source": [
    "### Evaluate the model using the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "710ee26097924153647ac432c8ade29383fe42f1"
   },
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68603a5e8cb5e507db95074a07b552a61fa48e11"
   },
   "outputs": [],
   "source": [
    "val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)\n",
    "print('val_top_2_acc:', val_top_2_acc)\n",
    "print('val_top_3_acc:', val_top_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "897f066da922d81fefa165a6b911a741c52ef7f5"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "\n",
    "val_loss, val_cat_acc, val_top_2_acc, val_top_3_acc = \\\n",
    "model.evaluate_generator(test_batches, \n",
    "                        steps=len(df_val))\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_cat_acc)\n",
    "print('val_top_2_acc:', val_top_2_acc)\n",
    "print('val_top_3_acc:', val_top_3_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74a66905f7a2d702f3d2aad9abf9fe114b96f0ff"
   },
   "outputs": [],
   "source": [
    "test_labels = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "_uuid": "53f4b22617285e923f336cdb2ffcbe1f9ff5e5db"
   },
   "outputs": [],
   "source": [
    "# We need these to plot the confusion matrix.\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5113e039e8384b96595751e084f0c5ed677080a"
   },
   "outputs": [],
   "source": [
    "# Print the label associated with each class\n",
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "701dafc5874aa60a054a74c04170cb7e8d750e94"
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "predictions = model.predict_generator(test_batches, steps=len(df_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dcce17ac0488ff90d29b11592c9226ed1bb210fb"
   },
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cfd9bdbbd27e27d9c5de7c6593527686445ea89"
   },
   "outputs": [],
   "source": [
    "# Source: Scikit Learn website\n",
    "# http://scikit-learn.org/stable/auto_examples/\n",
    "# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# selection-plot-confusion-matrix-py\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d499136cdb5fdf356515beb6e0cd1130ed584db"
   },
   "outputs": [],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "940b71bb2b37d847ba81dd67ca50c7fd5785fd35"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "97c6b493c368ff6565782c1bb15827f5d349ef79"
   },
   "outputs": [],
   "source": [
    "test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0ddbd33a93468075c64ba49188a6d272a5c7828f"
   },
   "outputs": [],
   "source": [
    "cm_plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n",
    "\n",
    "plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a85d7889e2bada2ebe3b84fc1571a89b1a66b7b0"
   },
   "source": [
    "### Convert the model from Keras to Tensorflowjs\n",
    "\n",
    "In order to convert this model the step is as follows:<br>\n",
    "1. Recreate the model using native Keras.<br>\n",
    "2. Use the command line conversion process to convert the model from Keras to Tensorflowjs.<br>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
